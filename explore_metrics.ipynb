{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e24823b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "import polars as pl\n",
    "\n",
    "OUTPUT_PATH = \"results/rag_results.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ded3359d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2_880, 11)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>item_index</th><th>model_name</th><th>prompt_variant</th><th>context_condition</th><th>question</th><th>gold_answer</th><th>context</th><th>documents</th><th>system_prompt</th><th>user_prompt</th><th>model_answer</th></tr><tr><td>i64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>list[struct[2]]</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>0</td><td>&quot;llama2:7b&quot;</td><td>&quot;langchain_base&quot;</td><td>&quot;relevant_only&quot;</td><td>&quot;What is the primary function o…</td><td>&quot;The mitochondria produce ATP a…</td><td>&quot;Mitochondria are membrane-boun…</td><td>[{&quot;Mitochondria are membrane-bound organelles responsible for producing ATP, the cell&#x27;s main energy currency.&quot;,&quot;relevant&quot;}, {&quot;Cells rely on mitochondria to convert nutrients into usable energy through the process of oxidative phosphorylation.&quot;,&quot;relevant&quot;}, {&quot;The mitochondria support cellular respiration, enabling the cell to perform functions requiring energy.&quot;,&quot;relevant&quot;}]</td><td>&quot;You are an assistant for quest…</td><td>&quot;Use the following pieces of re…</td><td>&quot;Mitochondria&#x27;s primary functio…</td></tr><tr><td>0</td><td>&quot;llama2:7b&quot;</td><td>&quot;langchain_system&quot;</td><td>&quot;relevant_only&quot;</td><td>&quot;What is the primary function o…</td><td>&quot;The mitochondria produce ATP a…</td><td>&quot;Mitochondria are membrane-boun…</td><td>[{&quot;Mitochondria are membrane-bound organelles responsible for producing ATP, the cell&#x27;s main energy currency.&quot;,&quot;relevant&quot;}, {&quot;Cells rely on mitochondria to convert nutrients into usable energy through the process of oxidative phosphorylation.&quot;,&quot;relevant&quot;}, {&quot;The mitochondria support cellular respiration, enabling the cell to perform functions requiring energy.&quot;,&quot;relevant&quot;}]</td><td>&quot;You are an assistant for quest…</td><td>null</td><td>&quot;The primary function of the mi…</td></tr><tr><td>0</td><td>&quot;llama2:7b&quot;</td><td>&quot;langchain_user&quot;</td><td>&quot;relevant_only&quot;</td><td>&quot;What is the primary function o…</td><td>&quot;The mitochondria produce ATP a…</td><td>&quot;Mitochondria are membrane-boun…</td><td>[{&quot;Mitochondria are membrane-bound organelles responsible for producing ATP, the cell&#x27;s main energy currency.&quot;,&quot;relevant&quot;}, {&quot;Cells rely on mitochondria to convert nutrients into usable energy through the process of oxidative phosphorylation.&quot;,&quot;relevant&quot;}, {&quot;The mitochondria support cellular respiration, enabling the cell to perform functions requiring energy.&quot;,&quot;relevant&quot;}]</td><td>null</td><td>&quot;You are an assistant for quest…</td><td>&quot;The primary function of mitoch…</td></tr><tr><td>0</td><td>&quot;llama2:7b&quot;</td><td>&quot;llamaindex_base&quot;</td><td>&quot;relevant_only&quot;</td><td>&quot;What is the primary function o…</td><td>&quot;The mitochondria produce ATP a…</td><td>&quot;Mitochondria are membrane-boun…</td><td>[{&quot;Mitochondria are membrane-bound organelles responsible for producing ATP, the cell&#x27;s main energy currency.&quot;,&quot;relevant&quot;}, {&quot;Cells rely on mitochondria to convert nutrients into usable energy through the process of oxidative phosphorylation.&quot;,&quot;relevant&quot;}, {&quot;The mitochondria support cellular respiration, enabling the cell to perform functions requiring energy.&quot;,&quot;relevant&quot;}]</td><td>&quot;If you don&#x27;t know the answer, …</td><td>&quot;We have provided context infor…</td><td>&quot;The primary function of mitoch…</td></tr><tr><td>0</td><td>&quot;llama2:7b&quot;</td><td>&quot;llamaindex_system&quot;</td><td>&quot;relevant_only&quot;</td><td>&quot;What is the primary function o…</td><td>&quot;The mitochondria produce ATP a…</td><td>&quot;Mitochondria are membrane-boun…</td><td>[{&quot;Mitochondria are membrane-bound organelles responsible for producing ATP, the cell&#x27;s main energy currency.&quot;,&quot;relevant&quot;}, {&quot;Cells rely on mitochondria to convert nutrients into usable energy through the process of oxidative phosphorylation.&quot;,&quot;relevant&quot;}, {&quot;The mitochondria support cellular respiration, enabling the cell to perform functions requiring energy.&quot;,&quot;relevant&quot;}]</td><td>&quot;If you don&#x27;t know the answer, …</td><td>null</td><td>&quot;The primary function of the mi…</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>19</td><td>&quot;qwen3:4b&quot;</td><td>&quot;claude_rag_system&quot;</td><td>&quot;mixed&quot;</td><td>&quot;What device measures atmospher…</td><td>&quot;A barometer measures atmospher…</td><td>&quot;Orange juice contains high lev…</td><td>[{&quot;Orange juice contains high levels of vitamin C.&quot;,&quot;irrelevant&quot;}, {&quot;The Amazon River flows eastward into the Atlantic Ocean.&quot;,&quot;irrelevant&quot;}, … {&quot;Bicycles use gears to change mechanical advantage.&quot;,&quot;irrelevant&quot;}]</td><td>&quot;Please remain faithful to the …</td><td>null</td><td>&quot;A barometer is the device that…</td></tr><tr><td>19</td><td>&quot;qwen3:4b&quot;</td><td>&quot;claude_rag_user&quot;</td><td>&quot;mixed&quot;</td><td>&quot;What device measures atmospher…</td><td>&quot;A barometer measures atmospher…</td><td>&quot;Bicycles use gears to change m…</td><td>[{&quot;Bicycles use gears to change mechanical advantage.&quot;,&quot;irrelevant&quot;}, {&quot;The moon orbits Earth approximately every 27 days.&quot;,&quot;irrelevant&quot;}, … {&quot;Mount Everest is located in the Himalayas.&quot;,&quot;irrelevant&quot;}]</td><td>null</td><td>&quot;Please remain faithful to the …</td><td>&quot;A barometer is an instrument u…</td></tr><tr><td>19</td><td>&quot;qwen3:4b&quot;</td><td>&quot;mastra_cot_base&quot;</td><td>&quot;mixed&quot;</td><td>&quot;What device measures atmospher…</td><td>&quot;A barometer measures atmospher…</td><td>&quot;Orange juice contains high lev…</td><td>[{&quot;Orange juice contains high levels of vitamin C.&quot;,&quot;irrelevant&quot;}, {&quot;The moon orbits Earth approximately every 27 days.&quot;,&quot;irrelevant&quot;}, … {&quot;A barometer is an instrument used to measure atmospheric pressure.&quot;,&quot;relevant&quot;}]</td><td>&quot;You are a helpful assistant th…</td><td>&quot;Question: {question}\n",
       "Context: …</td><td>&quot; A barometer is the device tha…</td></tr><tr><td>19</td><td>&quot;qwen3:4b&quot;</td><td>&quot;mastra_cot_system&quot;</td><td>&quot;mixed&quot;</td><td>&quot;What device measures atmospher…</td><td>&quot;A barometer measures atmospher…</td><td>&quot;Mount Everest is located in th…</td><td>[{&quot;Mount Everest is located in the Himalayas.&quot;,&quot;irrelevant&quot;}, {&quot;Barometers can be digital, aneroid, or mercury-based.&quot;,&quot;relevant&quot;}, … {&quot;A barometer is an instrument used to measure atmospheric pressure.&quot;,&quot;relevant&quot;}]</td><td>&quot;You are a helpful assistant th…</td><td>null</td><td>&quot; A barometer measures atmosphe…</td></tr><tr><td>19</td><td>&quot;qwen3:4b&quot;</td><td>&quot;mastra_cot_user&quot;</td><td>&quot;mixed&quot;</td><td>&quot;What device measures atmospher…</td><td>&quot;A barometer measures atmospher…</td><td>&quot;The Amazon River flows eastwar…</td><td>[{&quot;The Amazon River flows eastward into the Atlantic Ocean.&quot;,&quot;irrelevant&quot;}, {&quot;Orange juice contains high levels of vitamin C.&quot;,&quot;irrelevant&quot;}, … {&quot;Barometers can be digital, aneroid, or mercury-based.&quot;,&quot;relevant&quot;}]</td><td>null</td><td>&quot;You are a helpful assistant th…</td><td>&quot; A barometer is the device tha…</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2_880, 11)\n",
       "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
       "│ item_inde ┆ model_nam ┆ prompt_va ┆ context_c ┆ … ┆ documents ┆ system_pr ┆ user_prom ┆ model_an │\n",
       "│ x         ┆ e         ┆ riant     ┆ ondition  ┆   ┆ ---       ┆ ompt      ┆ pt        ┆ swer     │\n",
       "│ ---       ┆ ---       ┆ ---       ┆ ---       ┆   ┆ list[stru ┆ ---       ┆ ---       ┆ ---      │\n",
       "│ i64       ┆ str       ┆ str       ┆ str       ┆   ┆ ct[2]]    ┆ str       ┆ str       ┆ str      │\n",
       "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
       "│ 0         ┆ llama2:7b ┆ langchain ┆ relevant_ ┆ … ┆ [{\"Mitoch ┆ You are   ┆ Use the   ┆ Mitochon │\n",
       "│           ┆           ┆ _base     ┆ only      ┆   ┆ ondria    ┆ an        ┆ following ┆ dria's   │\n",
       "│           ┆           ┆           ┆           ┆   ┆ are membr ┆ assistant ┆ pieces of ┆ primary  │\n",
       "│           ┆           ┆           ┆           ┆   ┆ ane-b…    ┆ for       ┆ re…       ┆ functio… │\n",
       "│           ┆           ┆           ┆           ┆   ┆           ┆ quest…    ┆           ┆          │\n",
       "│ 0         ┆ llama2:7b ┆ langchain ┆ relevant_ ┆ … ┆ [{\"Mitoch ┆ You are   ┆ null      ┆ The      │\n",
       "│           ┆           ┆ _system   ┆ only      ┆   ┆ ondria    ┆ an        ┆           ┆ primary  │\n",
       "│           ┆           ┆           ┆           ┆   ┆ are membr ┆ assistant ┆           ┆ function │\n",
       "│           ┆           ┆           ┆           ┆   ┆ ane-b…    ┆ for       ┆           ┆ of the   │\n",
       "│           ┆           ┆           ┆           ┆   ┆           ┆ quest…    ┆           ┆ mi…      │\n",
       "│ 0         ┆ llama2:7b ┆ langchain ┆ relevant_ ┆ … ┆ [{\"Mitoch ┆ null      ┆ You are   ┆ The      │\n",
       "│           ┆           ┆ _user     ┆ only      ┆   ┆ ondria    ┆           ┆ an        ┆ primary  │\n",
       "│           ┆           ┆           ┆           ┆   ┆ are membr ┆           ┆ assistant ┆ function │\n",
       "│           ┆           ┆           ┆           ┆   ┆ ane-b…    ┆           ┆ for       ┆ of       │\n",
       "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆ quest…    ┆ mitoch…  │\n",
       "│ 0         ┆ llama2:7b ┆ llamainde ┆ relevant_ ┆ … ┆ [{\"Mitoch ┆ If you    ┆ We have   ┆ The      │\n",
       "│           ┆           ┆ x_base    ┆ only      ┆   ┆ ondria    ┆ don't     ┆ provided  ┆ primary  │\n",
       "│           ┆           ┆           ┆           ┆   ┆ are membr ┆ know the  ┆ context   ┆ function │\n",
       "│           ┆           ┆           ┆           ┆   ┆ ane-b…    ┆ answer, … ┆ infor…    ┆ of       │\n",
       "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ mitoch…  │\n",
       "│ 0         ┆ llama2:7b ┆ llamainde ┆ relevant_ ┆ … ┆ [{\"Mitoch ┆ If you    ┆ null      ┆ The      │\n",
       "│           ┆           ┆ x_system  ┆ only      ┆   ┆ ondria    ┆ don't     ┆           ┆ primary  │\n",
       "│           ┆           ┆           ┆           ┆   ┆ are membr ┆ know the  ┆           ┆ function │\n",
       "│           ┆           ┆           ┆           ┆   ┆ ane-b…    ┆ answer, … ┆           ┆ of the   │\n",
       "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ mi…      │\n",
       "│ …         ┆ …         ┆ …         ┆ …         ┆ … ┆ …         ┆ …         ┆ …         ┆ …        │\n",
       "│ 19        ┆ qwen3:4b  ┆ claude_ra ┆ mixed     ┆ … ┆ [{\"Orange ┆ Please    ┆ null      ┆ A barome │\n",
       "│           ┆           ┆ g_system  ┆           ┆   ┆ juice     ┆ remain    ┆           ┆ ter is   │\n",
       "│           ┆           ┆           ┆           ┆   ┆ contains  ┆ faithful  ┆           ┆ the      │\n",
       "│           ┆           ┆           ┆           ┆   ┆ high …    ┆ to the …  ┆           ┆ device   │\n",
       "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ that…    │\n",
       "│ 19        ┆ qwen3:4b  ┆ claude_ra ┆ mixed     ┆ … ┆ [{\"Bicycl ┆ null      ┆ Please    ┆ A barome │\n",
       "│           ┆           ┆ g_user    ┆           ┆   ┆ es use    ┆           ┆ remain    ┆ ter is   │\n",
       "│           ┆           ┆           ┆           ┆   ┆ gears to  ┆           ┆ faithful  ┆ an instr │\n",
       "│           ┆           ┆           ┆           ┆   ┆ chang…    ┆           ┆ to the …  ┆ ument u… │\n",
       "│ 19        ┆ qwen3:4b  ┆ mastra_co ┆ mixed     ┆ … ┆ [{\"Orange ┆ You are a ┆ Question: ┆ A barome │\n",
       "│           ┆           ┆ t_base    ┆           ┆   ┆ juice     ┆ helpful   ┆ {question ┆ ter is   │\n",
       "│           ┆           ┆           ┆           ┆   ┆ contains  ┆ assistant ┆ }         ┆ the      │\n",
       "│           ┆           ┆           ┆           ┆   ┆ high …    ┆ th…       ┆ Context:  ┆ device   │\n",
       "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆ …         ┆ tha…     │\n",
       "│ 19        ┆ qwen3:4b  ┆ mastra_co ┆ mixed     ┆ … ┆ [{\"Mount  ┆ You are a ┆ null      ┆ A barome │\n",
       "│           ┆           ┆ t_system  ┆           ┆   ┆ Everest   ┆ helpful   ┆           ┆ ter      │\n",
       "│           ┆           ┆           ┆           ┆   ┆ is        ┆ assistant ┆           ┆ measures │\n",
       "│           ┆           ┆           ┆           ┆   ┆ located   ┆ th…       ┆           ┆ atmosphe │\n",
       "│           ┆           ┆           ┆           ┆   ┆ in…       ┆           ┆           ┆ …        │\n",
       "│ 19        ┆ qwen3:4b  ┆ mastra_co ┆ mixed     ┆ … ┆ [{\"The    ┆ null      ┆ You are a ┆ A barome │\n",
       "│           ┆           ┆ t_user    ┆           ┆   ┆ Amazon    ┆           ┆ helpful   ┆ ter is   │\n",
       "│           ┆           ┆           ┆           ┆   ┆ River     ┆           ┆ assistant ┆ the      │\n",
       "│           ┆           ┆           ┆           ┆   ┆ flows     ┆           ┆ th…       ┆ device   │\n",
       "│           ┆           ┆           ┆           ┆   ┆ east…     ┆           ┆           ┆ tha…     │\n",
       "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pl.read_ndjson(OUTPUT_PATH)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a90af78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (3, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>context_condition</th><th>count</th></tr><tr><td>str</td><td>u32</td></tr></thead><tbody><tr><td>&quot;irrelevant_only&quot;</td><td>960</td></tr><tr><td>&quot;relevant_only&quot;</td><td>960</td></tr><tr><td>&quot;mixed&quot;</td><td>960</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (3, 2)\n",
       "┌───────────────────┬───────┐\n",
       "│ context_condition ┆ count │\n",
       "│ ---               ┆ ---   │\n",
       "│ str               ┆ u32   │\n",
       "╞═══════════════════╪═══════╡\n",
       "│ irrelevant_only   ┆ 960   │\n",
       "│ relevant_only     ┆ 960   │\n",
       "│ mixed             ┆ 960   │\n",
       "└───────────────────┴───────┘"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"context_condition\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "85660e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are grading an answer to a question.\n",
      "\n",
      "Question:\n",
      "whatever\n",
      "\n",
      "Gold/reference answer:\n",
      "I don't know OR The retrieved context does not contain information to answer the question\n",
      "\n",
      "Model's answer:\n",
      "something else\n",
      "\n",
      "Please do the following:\n",
      "1. Decide if the model's answer is factually correct with respect to the gold answer.\n",
      "2. Return ONLY a JSON object with two fields:\n",
      "   - \"correct\": a boolean (true or false)\n",
      "   - \"score\": a number between 0 and 1, where 1 means fully correct and 0 means completely incorrect.\n",
      "\n",
      "Do not add any explanation, just the JSON.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "JUDGE_PROMPT_TEMPLATE = \"\"\"\n",
    "You are grading an answer to a question.\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Gold/reference answer:\n",
    "{gold}\n",
    "\n",
    "Model's answer:\n",
    "{pred}\n",
    "\n",
    "Please do the following:\n",
    "1. Decide if the model's answer is factually correct with respect to the gold answer.\n",
    "2. Return ONLY a JSON object with two fields:\n",
    "   - \"correct\": a boolean (true or false)\n",
    "   - \"score\": a number between 0 and 1, where 1 means fully correct and 0 means completely incorrect.\n",
    "\n",
    "Do not add any explanation, just the JSON.\n",
    "\"\"\"\n",
    "\n",
    "print(JUDGE_PROMPT_TEMPLATE.format(gold= \"I don't know OR The retrieved context does not contain information to answer the question\", question=\"whatever\", pred=\"something else\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "823cca0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (960, 12)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>item_index</th><th>model_name</th><th>prompt_variant</th><th>context_condition</th><th>question</th><th>gold_answer</th><th>context</th><th>documents</th><th>system_prompt</th><th>user_prompt</th><th>model_answer</th><th>reference_answer</th></tr><tr><td>i64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>list[struct[2]]</td><td>str</td><td>str</td><td>str</td><td>list[str]</td></tr></thead><tbody><tr><td>0</td><td>&quot;llama2:7b&quot;</td><td>&quot;langchain_base&quot;</td><td>&quot;irrelevant_only&quot;</td><td>&quot;What is the primary function o…</td><td>&quot;The mitochondria produce ATP a…</td><td>&quot;The Eiffel Tower was completed…</td><td>[{&quot;The Eiffel Tower was completed in 1889 and stands as a symbol of Paris, France.&quot;,&quot;irrelevant&quot;}, {&quot;Solar panels convert sunlight into electricity using photovoltaic cells.&quot;,&quot;irrelevant&quot;}, … {&quot;Basketball was invented by James Naismith in 1891.&quot;,&quot;irrelevant&quot;}]</td><td>&quot;You are an assistant for quest…</td><td>&quot;Use the following pieces of re…</td><td>&quot;I don&#x27;t know the primary funct…</td><td>[&quot;I don&#x27;t know&quot;, &quot;The retrieved context does not contain information to answer the question&quot;]</td></tr><tr><td>0</td><td>&quot;llama2:7b&quot;</td><td>&quot;langchain_system&quot;</td><td>&quot;irrelevant_only&quot;</td><td>&quot;What is the primary function o…</td><td>&quot;The mitochondria produce ATP a…</td><td>&quot;The Eiffel Tower was completed…</td><td>[{&quot;The Eiffel Tower was completed in 1889 and stands as a symbol of Paris, France.&quot;,&quot;irrelevant&quot;}, {&quot;Solar panels convert sunlight into electricity using photovoltaic cells.&quot;,&quot;irrelevant&quot;}, … {&quot;Basketball was invented by James Naismith in 1891.&quot;,&quot;irrelevant&quot;}]</td><td>&quot;You are an assistant for quest…</td><td>null</td><td>&quot;I don&#x27;t know the primary funct…</td><td>[&quot;I don&#x27;t know&quot;, &quot;The retrieved context does not contain information to answer the question&quot;]</td></tr><tr><td>0</td><td>&quot;llama2:7b&quot;</td><td>&quot;langchain_user&quot;</td><td>&quot;irrelevant_only&quot;</td><td>&quot;What is the primary function o…</td><td>&quot;The mitochondria produce ATP a…</td><td>&quot;The Eiffel Tower was completed…</td><td>[{&quot;The Eiffel Tower was completed in 1889 and stands as a symbol of Paris, France.&quot;,&quot;irrelevant&quot;}, {&quot;Solar panels convert sunlight into electricity using photovoltaic cells.&quot;,&quot;irrelevant&quot;}, … {&quot;Basketball was invented by James Naismith in 1891.&quot;,&quot;irrelevant&quot;}]</td><td>null</td><td>&quot;You are an assistant for quest…</td><td>&quot;I don&#x27;t know the primary funct…</td><td>[&quot;I don&#x27;t know&quot;, &quot;The retrieved context does not contain information to answer the question&quot;]</td></tr><tr><td>0</td><td>&quot;llama2:7b&quot;</td><td>&quot;llamaindex_base&quot;</td><td>&quot;irrelevant_only&quot;</td><td>&quot;What is the primary function o…</td><td>&quot;The mitochondria produce ATP a…</td><td>&quot;The Eiffel Tower was completed…</td><td>[{&quot;The Eiffel Tower was completed in 1889 and stands as a symbol of Paris, France.&quot;,&quot;irrelevant&quot;}, {&quot;Solar panels convert sunlight into electricity using photovoltaic cells.&quot;,&quot;irrelevant&quot;}, … {&quot;Basketball was invented by James Naismith in 1891.&quot;,&quot;irrelevant&quot;}]</td><td>&quot;If you don&#x27;t know the answer, …</td><td>&quot;We have provided context infor…</td><td>&quot;I don&#x27;t know the answer to tha…</td><td>[&quot;I don&#x27;t know&quot;, &quot;The retrieved context does not contain information to answer the question&quot;]</td></tr><tr><td>0</td><td>&quot;llama2:7b&quot;</td><td>&quot;llamaindex_system&quot;</td><td>&quot;irrelevant_only&quot;</td><td>&quot;What is the primary function o…</td><td>&quot;The mitochondria produce ATP a…</td><td>&quot;The Eiffel Tower was completed…</td><td>[{&quot;The Eiffel Tower was completed in 1889 and stands as a symbol of Paris, France.&quot;,&quot;irrelevant&quot;}, {&quot;Solar panels convert sunlight into electricity using photovoltaic cells.&quot;,&quot;irrelevant&quot;}, … {&quot;Basketball was invented by James Naismith in 1891.&quot;,&quot;irrelevant&quot;}]</td><td>&quot;If you don&#x27;t know the answer, …</td><td>null</td><td>&quot;I don&#x27;t know the answer to tha…</td><td>[&quot;I don&#x27;t know&quot;, &quot;The retrieved context does not contain information to answer the question&quot;]</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>19</td><td>&quot;qwen3:4b&quot;</td><td>&quot;claude_rag_system&quot;</td><td>&quot;irrelevant_only&quot;</td><td>&quot;What device measures atmospher…</td><td>&quot;A barometer measures atmospher…</td><td>&quot;The Amazon River flows eastwar…</td><td>[{&quot;The Amazon River flows eastward into the Atlantic Ocean.&quot;,&quot;irrelevant&quot;}, {&quot;The moon orbits Earth approximately every 27 days.&quot;,&quot;irrelevant&quot;}, … {&quot;Bicycles use gears to change mechanical advantage.&quot;,&quot;irrelevant&quot;}]</td><td>&quot;Please remain faithful to the …</td><td>null</td><td>&quot;I don&#x27;t know.&quot;</td><td>[&quot;I don&#x27;t know&quot;, &quot;The retrieved context does not contain information to answer the question&quot;]</td></tr><tr><td>19</td><td>&quot;qwen3:4b&quot;</td><td>&quot;claude_rag_user&quot;</td><td>&quot;irrelevant_only&quot;</td><td>&quot;What device measures atmospher…</td><td>&quot;A barometer measures atmospher…</td><td>&quot;The Amazon River flows eastwar…</td><td>[{&quot;The Amazon River flows eastward into the Atlantic Ocean.&quot;,&quot;irrelevant&quot;}, {&quot;The moon orbits Earth approximately every 27 days.&quot;,&quot;irrelevant&quot;}, … {&quot;Bicycles use gears to change mechanical advantage.&quot;,&quot;irrelevant&quot;}]</td><td>null</td><td>&quot;Please remain faithful to the …</td><td>&quot;I don&#x27;t know.&quot;</td><td>[&quot;I don&#x27;t know&quot;, &quot;The retrieved context does not contain information to answer the question&quot;]</td></tr><tr><td>19</td><td>&quot;qwen3:4b&quot;</td><td>&quot;mastra_cot_base&quot;</td><td>&quot;irrelevant_only&quot;</td><td>&quot;What device measures atmospher…</td><td>&quot;A barometer measures atmospher…</td><td>&quot;The Amazon River flows eastwar…</td><td>[{&quot;The Amazon River flows eastward into the Atlantic Ocean.&quot;,&quot;irrelevant&quot;}, {&quot;The moon orbits Earth approximately every 27 days.&quot;,&quot;irrelevant&quot;}, … {&quot;Bicycles use gears to change mechanical advantage.&quot;,&quot;irrelevant&quot;}]</td><td>&quot;You are a helpful assistant th…</td><td>&quot;Question: {question}\n",
       "Context: …</td><td>&quot; The retrieved context does no…</td><td>[&quot;I don&#x27;t know&quot;, &quot;The retrieved context does not contain information to answer the question&quot;]</td></tr><tr><td>19</td><td>&quot;qwen3:4b&quot;</td><td>&quot;mastra_cot_system&quot;</td><td>&quot;irrelevant_only&quot;</td><td>&quot;What device measures atmospher…</td><td>&quot;A barometer measures atmospher…</td><td>&quot;The Amazon River flows eastwar…</td><td>[{&quot;The Amazon River flows eastward into the Atlantic Ocean.&quot;,&quot;irrelevant&quot;}, {&quot;The moon orbits Earth approximately every 27 days.&quot;,&quot;irrelevant&quot;}, … {&quot;Bicycles use gears to change mechanical advantage.&quot;,&quot;irrelevant&quot;}]</td><td>&quot;You are a helpful assistant th…</td><td>null</td><td>&quot; The retrieved context does no…</td><td>[&quot;I don&#x27;t know&quot;, &quot;The retrieved context does not contain information to answer the question&quot;]</td></tr><tr><td>19</td><td>&quot;qwen3:4b&quot;</td><td>&quot;mastra_cot_user&quot;</td><td>&quot;irrelevant_only&quot;</td><td>&quot;What device measures atmospher…</td><td>&quot;A barometer measures atmospher…</td><td>&quot;The Amazon River flows eastwar…</td><td>[{&quot;The Amazon River flows eastward into the Atlantic Ocean.&quot;,&quot;irrelevant&quot;}, {&quot;The moon orbits Earth approximately every 27 days.&quot;,&quot;irrelevant&quot;}, … {&quot;Bicycles use gears to change mechanical advantage.&quot;,&quot;irrelevant&quot;}]</td><td>null</td><td>&quot;You are a helpful assistant th…</td><td>&quot; The retrieved context does no…</td><td>[&quot;I don&#x27;t know&quot;, &quot;The retrieved context does not contain information to answer the question&quot;]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (960, 12)\n",
       "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
       "│ item_inde ┆ model_nam ┆ prompt_va ┆ context_c ┆ … ┆ system_pr ┆ user_prom ┆ model_ans ┆ referenc │\n",
       "│ x         ┆ e         ┆ riant     ┆ ondition  ┆   ┆ ompt      ┆ pt        ┆ wer       ┆ e_answer │\n",
       "│ ---       ┆ ---       ┆ ---       ┆ ---       ┆   ┆ ---       ┆ ---       ┆ ---       ┆ ---      │\n",
       "│ i64       ┆ str       ┆ str       ┆ str       ┆   ┆ str       ┆ str       ┆ str       ┆ list[str │\n",
       "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ ]        │\n",
       "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
       "│ 0         ┆ llama2:7b ┆ langchain ┆ irrelevan ┆ … ┆ You are   ┆ Use the   ┆ I don't   ┆ [\"I      │\n",
       "│           ┆           ┆ _base     ┆ t_only    ┆   ┆ an        ┆ following ┆ know the  ┆ don't    │\n",
       "│           ┆           ┆           ┆           ┆   ┆ assistant ┆ pieces of ┆ primary   ┆ know\",   │\n",
       "│           ┆           ┆           ┆           ┆   ┆ for       ┆ re…       ┆ funct…    ┆ \"The ret │\n",
       "│           ┆           ┆           ┆           ┆   ┆ quest…    ┆           ┆           ┆ rieve…   │\n",
       "│ 0         ┆ llama2:7b ┆ langchain ┆ irrelevan ┆ … ┆ You are   ┆ null      ┆ I don't   ┆ [\"I      │\n",
       "│           ┆           ┆ _system   ┆ t_only    ┆   ┆ an        ┆           ┆ know the  ┆ don't    │\n",
       "│           ┆           ┆           ┆           ┆   ┆ assistant ┆           ┆ primary   ┆ know\",   │\n",
       "│           ┆           ┆           ┆           ┆   ┆ for       ┆           ┆ funct…    ┆ \"The ret │\n",
       "│           ┆           ┆           ┆           ┆   ┆ quest…    ┆           ┆           ┆ rieve…   │\n",
       "│ 0         ┆ llama2:7b ┆ langchain ┆ irrelevan ┆ … ┆ null      ┆ You are   ┆ I don't   ┆ [\"I      │\n",
       "│           ┆           ┆ _user     ┆ t_only    ┆   ┆           ┆ an        ┆ know the  ┆ don't    │\n",
       "│           ┆           ┆           ┆           ┆   ┆           ┆ assistant ┆ primary   ┆ know\",   │\n",
       "│           ┆           ┆           ┆           ┆   ┆           ┆ for       ┆ funct…    ┆ \"The ret │\n",
       "│           ┆           ┆           ┆           ┆   ┆           ┆ quest…    ┆           ┆ rieve…   │\n",
       "│ 0         ┆ llama2:7b ┆ llamainde ┆ irrelevan ┆ … ┆ If you    ┆ We have   ┆ I don't   ┆ [\"I      │\n",
       "│           ┆           ┆ x_base    ┆ t_only    ┆   ┆ don't     ┆ provided  ┆ know the  ┆ don't    │\n",
       "│           ┆           ┆           ┆           ┆   ┆ know the  ┆ context   ┆ answer to ┆ know\",   │\n",
       "│           ┆           ┆           ┆           ┆   ┆ answer, … ┆ infor…    ┆ tha…      ┆ \"The ret │\n",
       "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ rieve…   │\n",
       "│ 0         ┆ llama2:7b ┆ llamainde ┆ irrelevan ┆ … ┆ If you    ┆ null      ┆ I don't   ┆ [\"I      │\n",
       "│           ┆           ┆ x_system  ┆ t_only    ┆   ┆ don't     ┆           ┆ know the  ┆ don't    │\n",
       "│           ┆           ┆           ┆           ┆   ┆ know the  ┆           ┆ answer to ┆ know\",   │\n",
       "│           ┆           ┆           ┆           ┆   ┆ answer, … ┆           ┆ tha…      ┆ \"The ret │\n",
       "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ rieve…   │\n",
       "│ …         ┆ …         ┆ …         ┆ …         ┆ … ┆ …         ┆ …         ┆ …         ┆ …        │\n",
       "│ 19        ┆ qwen3:4b  ┆ claude_ra ┆ irrelevan ┆ … ┆ Please    ┆ null      ┆ I don't   ┆ [\"I      │\n",
       "│           ┆           ┆ g_system  ┆ t_only    ┆   ┆ remain    ┆           ┆ know.     ┆ don't    │\n",
       "│           ┆           ┆           ┆           ┆   ┆ faithful  ┆           ┆           ┆ know\",   │\n",
       "│           ┆           ┆           ┆           ┆   ┆ to the …  ┆           ┆           ┆ \"The ret │\n",
       "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ rieve…   │\n",
       "│ 19        ┆ qwen3:4b  ┆ claude_ra ┆ irrelevan ┆ … ┆ null      ┆ Please    ┆ I don't   ┆ [\"I      │\n",
       "│           ┆           ┆ g_user    ┆ t_only    ┆   ┆           ┆ remain    ┆ know.     ┆ don't    │\n",
       "│           ┆           ┆           ┆           ┆   ┆           ┆ faithful  ┆           ┆ know\",   │\n",
       "│           ┆           ┆           ┆           ┆   ┆           ┆ to the …  ┆           ┆ \"The ret │\n",
       "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ rieve…   │\n",
       "│ 19        ┆ qwen3:4b  ┆ mastra_co ┆ irrelevan ┆ … ┆ You are a ┆ Question: ┆ The       ┆ [\"I      │\n",
       "│           ┆           ┆ t_base    ┆ t_only    ┆   ┆ helpful   ┆ {question ┆ retrieved ┆ don't    │\n",
       "│           ┆           ┆           ┆           ┆   ┆ assistant ┆ }         ┆ context   ┆ know\",   │\n",
       "│           ┆           ┆           ┆           ┆   ┆ th…       ┆ Context:  ┆ does no…  ┆ \"The ret │\n",
       "│           ┆           ┆           ┆           ┆   ┆           ┆ …         ┆           ┆ rieve…   │\n",
       "│ 19        ┆ qwen3:4b  ┆ mastra_co ┆ irrelevan ┆ … ┆ You are a ┆ null      ┆ The       ┆ [\"I      │\n",
       "│           ┆           ┆ t_system  ┆ t_only    ┆   ┆ helpful   ┆           ┆ retrieved ┆ don't    │\n",
       "│           ┆           ┆           ┆           ┆   ┆ assistant ┆           ┆ context   ┆ know\",   │\n",
       "│           ┆           ┆           ┆           ┆   ┆ th…       ┆           ┆ does no…  ┆ \"The ret │\n",
       "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ rieve…   │\n",
       "│ 19        ┆ qwen3:4b  ┆ mastra_co ┆ irrelevan ┆ … ┆ null      ┆ You are a ┆ The       ┆ [\"I      │\n",
       "│           ┆           ┆ t_user    ┆ t_only    ┆   ┆           ┆ helpful   ┆ retrieved ┆ don't    │\n",
       "│           ┆           ┆           ┆           ┆   ┆           ┆ assistant ┆ context   ┆ know\",   │\n",
       "│           ┆           ┆           ┆           ┆   ┆           ┆ th…       ┆ does no…  ┆ \"The ret │\n",
       "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ rieve…   │\n",
       "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idk = [\"I don't know\", \"The retrieved context does not contain information to answer the question\"]\n",
    "df.with_columns(\n",
    "    pl.when(\n",
    "        pl.col(\"context_condition\") == \"irrelevant_only\"\n",
    "    ).then(idk).otherwise(pl.concat_list(pl.col(\"gold_answer\"))).alias(\"reference_answer\")\n",
    ").filter(pl.col(\"context_condition\") == \"irrelevant_only\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c2ac931",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (20,)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>gold_answer</th></tr><tr><td>str</td></tr></thead><tbody><tr><td>&quot;The mitochondria produce ATP a…</td></tr><tr><td>&quot;Albert Einstein proposed the t…</td></tr><tr><td>&quot;Plants absorb carbon dioxide d…</td></tr><tr><td>&quot;The capital of Japan is Tokyo.&quot;</td></tr><tr><td>&quot;The element with the symbol &#x27;F…</td></tr><tr><td>&hellip;</td></tr><tr><td>&quot;The fastest land animal is the…</td></tr><tr><td>&quot;The Sahara Desert is located i…</td></tr><tr><td>&quot;George Washington was the firs…</td></tr><tr><td>&quot;Bees collect nectar to make ho…</td></tr><tr><td>&quot;A barometer measures atmospher…</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (20,)\n",
       "Series: 'gold_answer' [str]\n",
       "[\n",
       "\t\"The mitochondria produce ATP a…\n",
       "\t\"Albert Einstein proposed the t…\n",
       "\t\"Plants absorb carbon dioxide d…\n",
       "\t\"The capital of Japan is Tokyo.\"\n",
       "\t\"The element with the symbol 'F…\n",
       "\t…\n",
       "\t\"The fastest land animal is the…\n",
       "\t\"The Sahara Desert is located i…\n",
       "\t\"George Washington was the firs…\n",
       "\t\"Bees collect nectar to make ho…\n",
       "\t\"A barometer measures atmospher…\n",
       "]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"gold_answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "296d6a8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (20,)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>model_answer</th></tr><tr><td>str</td></tr></thead><tbody><tr><td>&quot;Mitochondria are responsible f…</td></tr><tr><td>&quot;Albert Einstein proposed the t…</td></tr><tr><td>&quot;During photosynthesis, plants …</td></tr><tr><td>&quot;Tokyo is the capital city of J…</td></tr><tr><td>&quot;The chemical symbol &#x27;Fe&#x27; stand…</td></tr><tr><td>&hellip;</td></tr><tr><td>&quot;The cheetah is the fastest lan…</td></tr><tr><td>&quot;The Sahara Desert is located o…</td></tr><tr><td>&quot;George Washington was the firs…</td></tr><tr><td>&quot;Bees collect nectar from flowe…</td></tr><tr><td>&quot;A barometer is an instrument u…</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (20,)\n",
       "Series: 'model_answer' [str]\n",
       "[\n",
       "\t\"Mitochondria are responsible f…\n",
       "\t\"Albert Einstein proposed the t…\n",
       "\t\"During photosynthesis, plants …\n",
       "\t\"Tokyo is the capital city of J…\n",
       "\t\"The chemical symbol 'Fe' stand…\n",
       "\t…\n",
       "\t\"The cheetah is the fastest lan…\n",
       "\t\"The Sahara Desert is located o…\n",
       "\t\"George Washington was the firs…\n",
       "\t\"Bees collect nectar from flowe…\n",
       "\t\"A barometer is an instrument u…\n",
       "]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"model_answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc4c0423",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The mitochondria produce ATP and serve as the cell’s primary source of energy.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(df[\"gold_answer\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5df1eacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "\n",
    "ans = next(iter(df[\"gold_answer\"]))\n",
    "resp = next(iter(df[\"model_answer\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d1c47ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rougeL': Score(precision=1.0, recall=0.6666666666666666, fmeasure=0.8)}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scorer.score_multi([\"A B C\", \"D E\"], \"A C\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5958bccd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2727272727272727"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scorer.score(ans, resp)['rougeL'].precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a88aff01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Score(precision=0.2727272727272727, recall=0.21428571428571427, fmeasure=0.23999999999999996)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scorer.score(ans, resp)['rougeL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dd649bbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Score(precision=0.2727272727272727, recall=0.21428571428571427, fmeasure=0.23999999999999996)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scorer.score_multi([ans], resp)['rougeL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0cd82571",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"gold_answer\"].len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f6947ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rougeL': Score(precision=0.5833333333333334, recall=0.5, fmeasure=0.5384615384615384)}\n",
      "{'rougeL': Score(precision=1.0, recall=1.0, fmeasure=1.0)}\n",
      "{'rougeL': Score(precision=0.4444444444444444, recall=0.6666666666666666, fmeasure=0.5333333333333333)}\n",
      "{'rougeL': Score(precision=0.5714285714285714, recall=0.6666666666666666, fmeasure=0.6153846153846153)}\n",
      "{'rougeL': Score(precision=0.5714285714285714, recall=0.5, fmeasure=0.5333333333333333)}\n",
      "{'rougeL': Score(precision=0.6363636363636364, recall=0.875, fmeasure=0.7368421052631579)}\n",
      "{'rougeL': Score(precision=1.0, recall=1.0, fmeasure=1.0)}\n",
      "{'rougeL': Score(precision=0.75, recall=1.0, fmeasure=0.8571428571428571)}\n",
      "{'rougeL': Score(precision=0.3, recall=0.5, fmeasure=0.37499999999999994)}\n",
      "{'rougeL': Score(precision=1.0, recall=1.0, fmeasure=1.0)}\n",
      "{'rougeL': Score(precision=1.0, recall=1.0, fmeasure=1.0)}\n",
      "{'rougeL': Score(precision=0.42857142857142855, recall=0.3333333333333333, fmeasure=0.375)}\n",
      "{'rougeL': Score(precision=0.5833333333333334, recall=1.0, fmeasure=0.7368421052631579)}\n",
      "{'rougeL': Score(precision=0.6, recall=1.0, fmeasure=0.7499999999999999)}\n",
      "{'rougeL': Score(precision=1.0, recall=1.0, fmeasure=1.0)}\n",
      "{'rougeL': Score(precision=0.5714285714285714, recall=0.5714285714285714, fmeasure=0.5714285714285714)}\n",
      "{'rougeL': Score(precision=0.5555555555555556, recall=0.7142857142857143, fmeasure=0.6250000000000001)}\n",
      "{'rougeL': Score(precision=0.6, recall=0.75, fmeasure=0.6666666666666665)}\n",
      "{'rougeL': Score(precision=0.4, recall=0.6666666666666666, fmeasure=0.5)}\n",
      "{'rougeL': Score(precision=0.5, recall=1.0, fmeasure=0.6666666666666666)}\n"
     ]
    }
   ],
   "source": [
    "for ans, resp in zip(df[\"gold_answer\"], df[\"model_answer\"]):\n",
    "    print(scorer.score(ans, resp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e568d76e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 {'rougeL': Score(precision=0.5833333333333334, recall=0.5, fmeasure=0.5384615384615384)}\n",
      "1 {'rougeL': Score(precision=1.0, recall=1.0, fmeasure=1.0)}\n",
      "2 {'rougeL': Score(precision=0.4444444444444444, recall=0.6666666666666666, fmeasure=0.5333333333333333)}\n",
      "3 {'rougeL': Score(precision=0.5714285714285714, recall=0.6666666666666666, fmeasure=0.6153846153846153)}\n",
      "4 {'rougeL': Score(precision=0.5714285714285714, recall=0.5, fmeasure=0.5333333333333333)}\n",
      "5 {'rougeL': Score(precision=0.6363636363636364, recall=0.875, fmeasure=0.7368421052631579)}\n",
      "6 {'rougeL': Score(precision=1.0, recall=1.0, fmeasure=1.0)}\n",
      "7 {'rougeL': Score(precision=0.75, recall=1.0, fmeasure=0.8571428571428571)}\n",
      "8 {'rougeL': Score(precision=0.3, recall=0.5, fmeasure=0.37499999999999994)}\n",
      "9 {'rougeL': Score(precision=1.0, recall=1.0, fmeasure=1.0)}\n",
      "10 {'rougeL': Score(precision=1.0, recall=1.0, fmeasure=1.0)}\n",
      "11 {'rougeL': Score(precision=0.42857142857142855, recall=0.3333333333333333, fmeasure=0.375)}\n",
      "12 {'rougeL': Score(precision=0.5833333333333334, recall=1.0, fmeasure=0.7368421052631579)}\n",
      "13 {'rougeL': Score(precision=0.6, recall=1.0, fmeasure=0.7499999999999999)}\n",
      "14 {'rougeL': Score(precision=1.0, recall=1.0, fmeasure=1.0)}\n",
      "15 {'rougeL': Score(precision=0.5714285714285714, recall=0.5714285714285714, fmeasure=0.5714285714285714)}\n",
      "16 {'rougeL': Score(precision=0.5555555555555556, recall=0.7142857142857143, fmeasure=0.6250000000000001)}\n",
      "17 {'rougeL': Score(precision=0.6, recall=0.75, fmeasure=0.6666666666666665)}\n",
      "18 {'rougeL': Score(precision=0.4, recall=0.6666666666666666, fmeasure=0.5)}\n",
      "19 {'rougeL': Score(precision=0.5, recall=1.0, fmeasure=0.6666666666666666)}\n"
     ]
    }
   ],
   "source": [
    "for i in range(df[\"gold_answer\"].len()):\n",
    "    ans = df[\"gold_answer\"][i]\n",
    "    resp = df[\"model_answer\"][i]\n",
    "    print(i, scorer.score(ans, resp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8a9441a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('George Washington was the first U.S. president.',\n",
       "  'George Washington was the first president of the United States.')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[17].select(pl.col(\"gold_answer\", \"model_answer\")).rows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0013c12a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48217931778851963"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.translate import bleu_score\n",
    "\n",
    "ans = next(iter(df[\"gold_answer\"]))\n",
    "resp = next(iter(df[\"model_answer\"]))\n",
    "bleu_score.sentence_bleu([ans], resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d315fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48217931778851963"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu_score.sentence_bleu([ans], resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f7ee0e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.48217931778851963\n",
      "1 1.0\n",
      "2 0.7115535295670167\n",
      "3 0.6736725254446679\n",
      "4 0.5181323157300696\n",
      "5 0.577497597217841\n",
      "6 1.0\n",
      "7 0.8136410108781797\n",
      "8 0.47886173800789306\n",
      "9 1.0\n",
      "10 1.0\n",
      "11 0.7317978729128142\n",
      "12 0.5441546696164566\n",
      "13 0.5865102576625455\n",
      "14 1.0\n",
      "15 0.9379094111501866\n",
      "16 0.6491555850394412\n",
      "17 0.6482123272483757\n",
      "18 0.45028900034315333\n",
      "19 0.5868292902204161\n"
     ]
    }
   ],
   "source": [
    "for i in range(df[\"gold_answer\"].len()):\n",
    "    ans = df[\"gold_answer\"][i]\n",
    "    resp = df[\"model_answer\"][i]\n",
    "    print(i, bleu_score.sentence_bleu([ans], resp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25d0714d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rougeL': Score(precision=0.5714285714285714, recall=0.5714285714285714, fmeasure=0.5714285714285714)}\n",
      "0.9379094111501866\n"
     ]
    }
   ],
   "source": [
    "ans = df[\"gold_answer\"][15]\n",
    "resp = df[\"model_answer\"][15]\n",
    "\n",
    "print(scorer.score(ans, resp))\n",
    "print(bleu_score.sentence_bleu([ans], resp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b094bbfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The fastest land animal is the cheetah.',\n",
       "  'The cheetah is the fastest land animal.')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[15].select(pl.col(\"gold_answer\", \"model_answer\")).rows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "444d8d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_score import score as score_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b156087",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10bbb35dd4eb473697bb6ce286ddb1f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de44783d89ba4d299739e8e9d8bf4cbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.02 seconds, 0.50 sentences/sec\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([0.8981]), tensor([0.9201]), tensor([0.9090]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans = next(iter(df[\"gold_answer\"]))\n",
    "resp = next(iter(df[\"model_answer\"]))\n",
    "\n",
    "score_bert([resp], [ans], lang='en', verbose=True, use_fast_tokenizer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dec7328d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The mitochondria produce ATP and serve as the cell’s primary source of energy.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e1547be9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Mitochondria are responsible for producing ATP, the cell's main energy currency.\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e623924",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "851ee8165f934e838c9cb2b1554a2df3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9983df09f4843498e6c02ddc9420de6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.03 seconds, 68.87 sentences/sec\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([0.9593]), tensor([0.9593]), tensor([0.9594]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testAns = \"Abraham Lincoln was the 16th president of the United States\"\n",
    "testResp = \"Abraham Lincoln was not the 16th president of the United States\"\n",
    "\n",
    "score_bert([testResp], [[testAns, \"Abraham Lincoln wasn't the 16th president of the United States\"]], rescale_with_baseline=True, lang='en', verbose=True, use_fast_tokenizer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a61b78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "(tensor([0.8060]), tensor([0.8888]), tensor([0.8473]))\n",
    "(tensor([0.9593]), tensor([0.9593]), tensor([0.9594]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "064a74db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bc08200c8bc4638b539ee0490e90d7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1acf9a4555a54730b5bb5803103137f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.02 seconds, 47.43 sentences/sec\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([0.7910]), tensor([0.8656]), tensor([0.8283]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testAns = \"Abraham Lincoln was the 16th president of the United States\"\n",
    "testResp = \"Lincoln was the 16th president of the US\"\n",
    "\n",
    "score_bert([testAns], [testResp], rescale_with_baseline=True, lang='en', verbose=True, use_fast_tokenizer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e87ad306",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f374f1e85c8d4f3595f16de4cc5ddac1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a80776b9ce614085b7a63e641885f21a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.02 seconds, 49.47 sentences/sec\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([0.7687]), tensor([0.8267]), tensor([0.7979]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testAns = \"Abraham Lincoln was the 16th president of the United States\"\n",
    "testResp = \"Lincoln was the 15th president of the US\"\n",
    "\n",
    "score_bert([testAns], [testResp], rescale_with_baseline=True, lang='en', verbose=True, use_fast_tokenizer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9d61117d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b7c060958254caab18cac825a84cddb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96333371fde648f6ba488702b2f3b770",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.03 seconds, 62.06 sentences/sec\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([0.7687, 0.0674]), tensor([0.8267, 0.0707]), tensor([0.7979, 0.0705]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_bert([testAns, \"Howdy\"], [testResp, \"Hello\"], rescale_with_baseline=True, lang='en', verbose=True, use_fast_tokenizer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "edc17876",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Mitochondria are responsible for producing ATP, the cell's main energy currency.\",\n",
       " 'Albert Einstein proposed the theory of general relativity.',\n",
       " 'During photosynthesis, plants absorb carbon dioxide from the air.',\n",
       " 'Tokyo is the capital city of Japan.',\n",
       " \"The chemical symbol 'Fe' stands for iron.\",\n",
       " 'The heart is responsible for pumping blood throughout the human body.',\n",
       " 'Jupiter is the largest planet in the Solar System.',\n",
       " \"William Shakespeare wrote the play 'Romeo and Juliet'.\",\n",
       " 'The freezing point of water in Celsius is 0 degrees.',\n",
       " 'The Pacific Ocean is the largest ocean on Earth.',\n",
       " 'Mars is known as the Red Planet.',\n",
       " 'Leonardo da Vinci painted the Mona Lisa.',\n",
       " 'DNA, or deoxyribonucleic acid, carries genetic information in nearly all living organisms.',\n",
       " 'Paper was invented in ancient China during the Han Dynasty.',\n",
       " 'The chemical formula for water is H2O.',\n",
       " 'The cheetah is the fastest land animal.',\n",
       " 'The Sahara Desert is located on the African continent.',\n",
       " 'George Washington was the first president of the United States.',\n",
       " 'Bees collect nectar from flowers and convert it into honey.',\n",
       " 'A barometer is an instrument used to measure atmospheric pressure.']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"model_answer\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6ff8f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d063a0e3e544bee8178ba0c0e926443",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "199d35ae0b1746a494fbaeb785928cda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.16 seconds, 122.61 sentences/sec\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([0.6008, 1.0000, 0.8124, 0.7886, 0.6499, 0.8843, 1.0000, 0.9622, 0.6501,\n",
       "         1.0000, 1.0000, 0.7281, 0.8850, 0.9572, 1.0000, 0.8587, 0.9351, 0.9156,\n",
       "         0.8130, 0.8570]),\n",
       " tensor([0.6365, 1.0000, 0.6711, 0.7392, 0.6516, 0.7689, 1.0000, 0.8879, 0.4480,\n",
       "         1.0000, 1.0000, 0.8138, 0.2658, 0.7732, 1.0000, 0.8850, 0.8602, 0.8138,\n",
       "         0.6348, 0.6776]),\n",
       " tensor([0.6193, 1.0000, 0.7413, 0.7642, 0.6513, 0.8263, 1.0000, 0.9249, 0.5479,\n",
       "         1.0000, 1.0000, 0.7710, 0.5587, 0.8640, 1.0000, 0.8720, 0.8975, 0.8644,\n",
       "         0.7229, 0.7663]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_bert(df[\"gold_answer\"].to_list(), df[\"model_answer\"].to_list(), rescale_with_baseline=True, lang='en', verbose=True, use_fast_tokenizer=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cac0d03",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
