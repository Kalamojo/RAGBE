{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e24823b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "import polars as pl\n",
    "\n",
    "OUTPUT_PATH = \"results/rag_results.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ded3359d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (20, 8)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>item_index</th><th>prompt_variant</th><th>question</th><th>gold_answer</th><th>context</th><th>documents</th><th>system_prompt</th><th>model_answer</th></tr><tr><td>i64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>list[struct[2]]</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>0</td><td>&quot;baseline&quot;</td><td>&quot;What is the primary function o…</td><td>&quot;The mitochondria produce ATP a…</td><td>&quot;Shakespeare wrote tragedies su…</td><td>[{&quot;Shakespeare wrote tragedies such as Macbeth and Romeo and Juliet.&quot;,&quot;irrelevant&quot;}, {&quot;The Eiffel Tower was completed in 1889 and stands as a symbol of Paris, France.&quot;,&quot;irrelevant&quot;}, … {&quot;Basketball was invented by James Naismith in 1891.&quot;,&quot;irrelevant&quot;}]</td><td>&quot;You are an assistant for quest…</td><td>&quot;Mitochondria are responsible f…</td></tr><tr><td>1</td><td>&quot;baseline&quot;</td><td>&quot;Who proposed the theory of gen…</td><td>&quot;Albert Einstein proposed the t…</td><td>&quot;Photosynthesis converts light …</td><td>[{&quot;Photosynthesis converts light into chemical energy in plants.&quot;,&quot;irrelevant&quot;}, {&quot;The ancient Romans built aqueducts to transport water across great distances.&quot;,&quot;irrelevant&quot;}, … {&quot;Python is a high-level programming language known for its readability.&quot;,&quot;irrelevant&quot;}]</td><td>&quot;You are an assistant for quest…</td><td>&quot;Albert Einstein proposed the t…</td></tr><tr><td>2</td><td>&quot;baseline&quot;</td><td>&quot;What gas do plants absorb duri…</td><td>&quot;Plants absorb carbon dioxide d…</td><td>&quot;The human heart contains four …</td><td>[{&quot;The human heart contains four chambers: two atria and two ventricles.&quot;,&quot;irrelevant&quot;}, {&quot;Carbon dioxide, water, and sunlight are essential components for producing glucose in plants.&quot;,&quot;relevant&quot;}, … {&quot;The Pacific Ocean is the largest ocean on Earth.&quot;,&quot;irrelevant&quot;}]</td><td>&quot;You are an assistant for quest…</td><td>&quot;During photosynthesis, plants …</td></tr><tr><td>3</td><td>&quot;baseline&quot;</td><td>&quot;What is the capital of Japan?&quot;</td><td>&quot;The capital of Japan is Tokyo.&quot;</td><td>&quot;The Pythagorean theorem relate…</td><td>[{&quot;The Pythagorean theorem relates the sides of a right triangle.&quot;,&quot;irrelevant&quot;}, {&quot;Jupiter has over 70 known moons orbiting it.&quot;,&quot;irrelevant&quot;}, … {&quot;Diamonds are composed of carbon atoms arranged in a crystal lattice.&quot;,&quot;irrelevant&quot;}]</td><td>&quot;You are an assistant for quest…</td><td>&quot;Tokyo is the capital city of J…</td></tr><tr><td>4</td><td>&quot;baseline&quot;</td><td>&quot;Which element has the chemical…</td><td>&quot;The element with the symbol &#x27;F…</td><td>&quot;The symbol Fe originates from …</td><td>[{&quot;The symbol Fe originates from the Latin word &#x27;ferrum&#x27;, meaning iron.&quot;,&quot;relevant&quot;}, {&quot;The Taj Mahal was built during the Mughal Empire.&quot;,&quot;irrelevant&quot;}, … {&quot;Whales are the largest mammals on Earth.&quot;,&quot;irrelevant&quot;}]</td><td>&quot;You are an assistant for quest…</td><td>&quot;The chemical symbol &#x27;Fe&#x27; stand…</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>15</td><td>&quot;baseline&quot;</td><td>&quot;What is the fastest land anima…</td><td>&quot;The fastest land animal is the…</td><td>&quot;Diamonds are formed under high…</td><td>[{&quot;Diamonds are formed under high pressure deep underground.&quot;,&quot;irrelevant&quot;}, {&quot;Bats are the only mammals capable of sustained flight.&quot;,&quot;irrelevant&quot;}, … {&quot;Chocolate is made from cacao seeds.&quot;,&quot;irrelevant&quot;}]</td><td>&quot;You are an assistant for quest…</td><td>&quot;The cheetah is the fastest lan…</td></tr><tr><td>16</td><td>&quot;baseline&quot;</td><td>&quot;Which continent is the Sahara …</td><td>&quot;The Sahara Desert is located i…</td><td>&quot;The Pythagorean theorem relate…</td><td>[{&quot;The Pythagorean theorem relates the sides of a right triangle.&quot;,&quot;irrelevant&quot;}, {&quot;Apples grow on deciduous trees.&quot;,&quot;irrelevant&quot;}, … {&quot;Butterflies have four wings.&quot;,&quot;irrelevant&quot;}]</td><td>&quot;You are an assistant for quest…</td><td>&quot;The Sahara Desert is located o…</td></tr><tr><td>17</td><td>&quot;baseline&quot;</td><td>&quot;Who was the first president of…</td><td>&quot;George Washington was the firs…</td><td>&quot;George Washington served as th…</td><td>[{&quot;George Washington served as the first president of the United States from 1789 to 1797.&quot;,&quot;relevant&quot;}, {&quot;Basketball was invented in the 19th century.&quot;,&quot;irrelevant&quot;}, … {&quot;Washington played a key role in the American Revolution and the founding of the nation.&quot;,&quot;relevant&quot;}]</td><td>&quot;You are an assistant for quest…</td><td>&quot;George Washington was the firs…</td></tr><tr><td>18</td><td>&quot;baseline&quot;</td><td>&quot;What do bees collect to make h…</td><td>&quot;Bees collect nectar to make ho…</td><td>&quot;Bees collect nectar from flowe…</td><td>[{&quot;Bees collect nectar from flowers and convert it into honey.&quot;,&quot;relevant&quot;}, {&quot;Worker bees gather nectar and store it in the hive, where enzymes break it down.&quot;,&quot;relevant&quot;}, … {&quot;Salt dissolves readily in water.&quot;,&quot;irrelevant&quot;}]</td><td>&quot;You are an assistant for quest…</td><td>&quot;Bees collect nectar from flowe…</td></tr><tr><td>19</td><td>&quot;baseline&quot;</td><td>&quot;What device measures atmospher…</td><td>&quot;A barometer measures atmospher…</td><td>&quot;A triangle&#x27;s angles add up to …</td><td>[{&quot;A triangle&#x27;s angles add up to 180 degrees.&quot;,&quot;irrelevant&quot;}, {&quot;Lions live in prides on the African savanna.&quot;,&quot;irrelevant&quot;}, … {&quot;Meteorologists use barometers to forecast weather based on pressure changes.&quot;,&quot;relevant&quot;}]</td><td>&quot;You are an assistant for quest…</td><td>&quot;A barometer is an instrument u…</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (20, 8)\n",
       "┌────────────┬────────────┬────────────┬───────────┬───────────┬───────────┬───────────┬───────────┐\n",
       "│ item_index ┆ prompt_var ┆ question   ┆ gold_answ ┆ context   ┆ documents ┆ system_pr ┆ model_ans │\n",
       "│ ---        ┆ iant       ┆ ---        ┆ er        ┆ ---       ┆ ---       ┆ ompt      ┆ wer       │\n",
       "│ i64        ┆ ---        ┆ str        ┆ ---       ┆ str       ┆ list[stru ┆ ---       ┆ ---       │\n",
       "│            ┆ str        ┆            ┆ str       ┆           ┆ ct[2]]    ┆ str       ┆ str       │\n",
       "╞════════════╪════════════╪════════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╡\n",
       "│ 0          ┆ baseline   ┆ What is    ┆ The mitoc ┆ Shakespea ┆ [{\"Shakes ┆ You are   ┆ Mitochond │\n",
       "│            ┆            ┆ the        ┆ hondria   ┆ re wrote  ┆ peare     ┆ an        ┆ ria are   │\n",
       "│            ┆            ┆ primary    ┆ produce   ┆ tragedies ┆ wrote tra ┆ assistant ┆ responsib │\n",
       "│            ┆            ┆ function   ┆ ATP a…    ┆ su…       ┆ gedies…   ┆ for       ┆ le f…     │\n",
       "│            ┆            ┆ o…         ┆           ┆           ┆           ┆ quest…    ┆           │\n",
       "│ 1          ┆ baseline   ┆ Who        ┆ Albert    ┆ Photosynt ┆ [{\"Photos ┆ You are   ┆ Albert    │\n",
       "│            ┆            ┆ proposed   ┆ Einstein  ┆ hesis     ┆ ynthesis  ┆ an        ┆ Einstein  │\n",
       "│            ┆            ┆ the theory ┆ proposed  ┆ converts  ┆ converts  ┆ assistant ┆ proposed  │\n",
       "│            ┆            ┆ of gen…    ┆ the t…    ┆ light …   ┆ lig…      ┆ for       ┆ the t…    │\n",
       "│            ┆            ┆            ┆           ┆           ┆           ┆ quest…    ┆           │\n",
       "│ 2          ┆ baseline   ┆ What gas   ┆ Plants    ┆ The human ┆ [{\"The    ┆ You are   ┆ During    │\n",
       "│            ┆            ┆ do plants  ┆ absorb    ┆ heart     ┆ human     ┆ an        ┆ photosynt │\n",
       "│            ┆            ┆ absorb     ┆ carbon    ┆ contains  ┆ heart     ┆ assistant ┆ hesis,    │\n",
       "│            ┆            ┆ duri…      ┆ dioxide   ┆ four …    ┆ contains  ┆ for       ┆ plants …  │\n",
       "│            ┆            ┆            ┆ d…        ┆           ┆ fo…       ┆ quest…    ┆           │\n",
       "│ 3          ┆ baseline   ┆ What is    ┆ The       ┆ The Pytha ┆ [{\"The    ┆ You are   ┆ Tokyo is  │\n",
       "│            ┆            ┆ the        ┆ capital   ┆ gorean    ┆ Pythagore ┆ an        ┆ the       │\n",
       "│            ┆            ┆ capital of ┆ of Japan  ┆ theorem   ┆ an        ┆ assistant ┆ capital   │\n",
       "│            ┆            ┆ Japan?     ┆ is Tokyo. ┆ relate…   ┆ theorem   ┆ for       ┆ city of   │\n",
       "│            ┆            ┆            ┆           ┆           ┆ rel…      ┆ quest…    ┆ J…        │\n",
       "│ 4          ┆ baseline   ┆ Which      ┆ The       ┆ The       ┆ [{\"The    ┆ You are   ┆ The       │\n",
       "│            ┆            ┆ element    ┆ element   ┆ symbol Fe ┆ symbol Fe ┆ an        ┆ chemical  │\n",
       "│            ┆            ┆ has the    ┆ with the  ┆ originate ┆ originate ┆ assistant ┆ symbol    │\n",
       "│            ┆            ┆ chemical…  ┆ symbol    ┆ s from …  ┆ s fr…     ┆ for       ┆ 'Fe'      │\n",
       "│            ┆            ┆            ┆ 'F…       ┆           ┆           ┆ quest…    ┆ stand…    │\n",
       "│ …          ┆ …          ┆ …          ┆ …         ┆ …         ┆ …         ┆ …         ┆ …         │\n",
       "│ 15         ┆ baseline   ┆ What is    ┆ The       ┆ Diamonds  ┆ [{\"Diamon ┆ You are   ┆ The       │\n",
       "│            ┆            ┆ the        ┆ fastest   ┆ are       ┆ ds are    ┆ an        ┆ cheetah   │\n",
       "│            ┆            ┆ fastest    ┆ land      ┆ formed    ┆ formed    ┆ assistant ┆ is the    │\n",
       "│            ┆            ┆ land       ┆ animal is ┆ under     ┆ under h…  ┆ for       ┆ fastest   │\n",
       "│            ┆            ┆ anima…     ┆ the…      ┆ high…     ┆           ┆ quest…    ┆ lan…      │\n",
       "│ 16         ┆ baseline   ┆ Which      ┆ The       ┆ The Pytha ┆ [{\"The    ┆ You are   ┆ The       │\n",
       "│            ┆            ┆ continent  ┆ Sahara    ┆ gorean    ┆ Pythagore ┆ an        ┆ Sahara    │\n",
       "│            ┆            ┆ is the     ┆ Desert is ┆ theorem   ┆ an        ┆ assistant ┆ Desert is │\n",
       "│            ┆            ┆ Sahara …   ┆ located   ┆ relate…   ┆ theorem   ┆ for       ┆ located   │\n",
       "│            ┆            ┆            ┆ i…        ┆           ┆ rel…      ┆ quest…    ┆ o…        │\n",
       "│ 17         ┆ baseline   ┆ Who was    ┆ George    ┆ George    ┆ [{\"George ┆ You are   ┆ George    │\n",
       "│            ┆            ┆ the first  ┆ Washingto ┆ Washingto ┆ Washingto ┆ an        ┆ Washingto │\n",
       "│            ┆            ┆ president  ┆ n was the ┆ n served  ┆ n served  ┆ assistant ┆ n was the │\n",
       "│            ┆            ┆ of…        ┆ firs…     ┆ as th…    ┆ as…       ┆ for       ┆ firs…     │\n",
       "│            ┆            ┆            ┆           ┆           ┆           ┆ quest…    ┆           │\n",
       "│ 18         ┆ baseline   ┆ What do    ┆ Bees      ┆ Bees      ┆ [{\"Bees   ┆ You are   ┆ Bees      │\n",
       "│            ┆            ┆ bees       ┆ collect   ┆ collect   ┆ collect   ┆ an        ┆ collect   │\n",
       "│            ┆            ┆ collect to ┆ nectar to ┆ nectar    ┆ nectar    ┆ assistant ┆ nectar    │\n",
       "│            ┆            ┆ make h…    ┆ make ho…  ┆ from      ┆ from fl…  ┆ for       ┆ from      │\n",
       "│            ┆            ┆            ┆           ┆ flowe…    ┆           ┆ quest…    ┆ flowe…    │\n",
       "│ 19         ┆ baseline   ┆ What       ┆ A         ┆ A triangl ┆ [{\"A tria ┆ You are   ┆ A         │\n",
       "│            ┆            ┆ device     ┆ barometer ┆ e's       ┆ ngle's    ┆ an        ┆ barometer │\n",
       "│            ┆            ┆ measures   ┆ measures  ┆ angles    ┆ angles    ┆ assistant ┆ is an ins │\n",
       "│            ┆            ┆ atmospher… ┆ atmospher ┆ add up to ┆ add up …  ┆ for       ┆ trument   │\n",
       "│            ┆            ┆            ┆ …         ┆ …         ┆           ┆ quest…    ┆ u…        │\n",
       "└────────────┴────────────┴────────────┴───────────┴───────────┴───────────┴───────────┴───────────┘"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pl.read_ndjson(OUTPUT_PATH)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c2ac931",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (20,)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>gold_answer</th></tr><tr><td>str</td></tr></thead><tbody><tr><td>&quot;The mitochondria produce ATP a…</td></tr><tr><td>&quot;Albert Einstein proposed the t…</td></tr><tr><td>&quot;Plants absorb carbon dioxide d…</td></tr><tr><td>&quot;The capital of Japan is Tokyo.&quot;</td></tr><tr><td>&quot;The element with the symbol &#x27;F…</td></tr><tr><td>&hellip;</td></tr><tr><td>&quot;The fastest land animal is the…</td></tr><tr><td>&quot;The Sahara Desert is located i…</td></tr><tr><td>&quot;George Washington was the firs…</td></tr><tr><td>&quot;Bees collect nectar to make ho…</td></tr><tr><td>&quot;A barometer measures atmospher…</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (20,)\n",
       "Series: 'gold_answer' [str]\n",
       "[\n",
       "\t\"The mitochondria produce ATP a…\n",
       "\t\"Albert Einstein proposed the t…\n",
       "\t\"Plants absorb carbon dioxide d…\n",
       "\t\"The capital of Japan is Tokyo.\"\n",
       "\t\"The element with the symbol 'F…\n",
       "\t…\n",
       "\t\"The fastest land animal is the…\n",
       "\t\"The Sahara Desert is located i…\n",
       "\t\"George Washington was the firs…\n",
       "\t\"Bees collect nectar to make ho…\n",
       "\t\"A barometer measures atmospher…\n",
       "]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"gold_answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "296d6a8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (20,)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>model_answer</th></tr><tr><td>str</td></tr></thead><tbody><tr><td>&quot;Mitochondria are responsible f…</td></tr><tr><td>&quot;Albert Einstein proposed the t…</td></tr><tr><td>&quot;During photosynthesis, plants …</td></tr><tr><td>&quot;Tokyo is the capital city of J…</td></tr><tr><td>&quot;The chemical symbol &#x27;Fe&#x27; stand…</td></tr><tr><td>&hellip;</td></tr><tr><td>&quot;The cheetah is the fastest lan…</td></tr><tr><td>&quot;The Sahara Desert is located o…</td></tr><tr><td>&quot;George Washington was the firs…</td></tr><tr><td>&quot;Bees collect nectar from flowe…</td></tr><tr><td>&quot;A barometer is an instrument u…</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (20,)\n",
       "Series: 'model_answer' [str]\n",
       "[\n",
       "\t\"Mitochondria are responsible f…\n",
       "\t\"Albert Einstein proposed the t…\n",
       "\t\"During photosynthesis, plants …\n",
       "\t\"Tokyo is the capital city of J…\n",
       "\t\"The chemical symbol 'Fe' stand…\n",
       "\t…\n",
       "\t\"The cheetah is the fastest lan…\n",
       "\t\"The Sahara Desert is located o…\n",
       "\t\"George Washington was the firs…\n",
       "\t\"Bees collect nectar from flowe…\n",
       "\t\"A barometer is an instrument u…\n",
       "]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"model_answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc4c0423",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The mitochondria produce ATP and serve as the cell’s primary source of energy.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(df[\"gold_answer\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5df1eacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "\n",
    "ans = next(iter(df[\"gold_answer\"]))\n",
    "resp = next(iter(df[\"model_answer\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d1c47ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rougeL': Score(precision=1.0, recall=0.6666666666666666, fmeasure=0.8)}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scorer.score_multi([\"A B C\", \"D E\"], \"A C\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5958bccd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scorer.score(ans, resp)['rougeL'].precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0cd82571",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"gold_answer\"].len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f6947ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rougeL': Score(precision=0.5833333333333334, recall=0.5, fmeasure=0.5384615384615384)}\n",
      "{'rougeL': Score(precision=1.0, recall=1.0, fmeasure=1.0)}\n",
      "{'rougeL': Score(precision=0.4444444444444444, recall=0.6666666666666666, fmeasure=0.5333333333333333)}\n",
      "{'rougeL': Score(precision=0.5714285714285714, recall=0.6666666666666666, fmeasure=0.6153846153846153)}\n",
      "{'rougeL': Score(precision=0.5714285714285714, recall=0.5, fmeasure=0.5333333333333333)}\n",
      "{'rougeL': Score(precision=0.6363636363636364, recall=0.875, fmeasure=0.7368421052631579)}\n",
      "{'rougeL': Score(precision=1.0, recall=1.0, fmeasure=1.0)}\n",
      "{'rougeL': Score(precision=0.75, recall=1.0, fmeasure=0.8571428571428571)}\n",
      "{'rougeL': Score(precision=0.3, recall=0.5, fmeasure=0.37499999999999994)}\n",
      "{'rougeL': Score(precision=1.0, recall=1.0, fmeasure=1.0)}\n",
      "{'rougeL': Score(precision=1.0, recall=1.0, fmeasure=1.0)}\n",
      "{'rougeL': Score(precision=0.42857142857142855, recall=0.3333333333333333, fmeasure=0.375)}\n",
      "{'rougeL': Score(precision=0.5833333333333334, recall=1.0, fmeasure=0.7368421052631579)}\n",
      "{'rougeL': Score(precision=0.6, recall=1.0, fmeasure=0.7499999999999999)}\n",
      "{'rougeL': Score(precision=1.0, recall=1.0, fmeasure=1.0)}\n",
      "{'rougeL': Score(precision=0.5714285714285714, recall=0.5714285714285714, fmeasure=0.5714285714285714)}\n",
      "{'rougeL': Score(precision=0.5555555555555556, recall=0.7142857142857143, fmeasure=0.6250000000000001)}\n",
      "{'rougeL': Score(precision=0.6, recall=0.75, fmeasure=0.6666666666666665)}\n",
      "{'rougeL': Score(precision=0.4, recall=0.6666666666666666, fmeasure=0.5)}\n",
      "{'rougeL': Score(precision=0.5, recall=1.0, fmeasure=0.6666666666666666)}\n"
     ]
    }
   ],
   "source": [
    "for ans, resp in zip(df[\"gold_answer\"], df[\"model_answer\"]):\n",
    "    print(scorer.score(ans, resp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e568d76e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 {'rougeL': Score(precision=0.5833333333333334, recall=0.5, fmeasure=0.5384615384615384)}\n",
      "1 {'rougeL': Score(precision=1.0, recall=1.0, fmeasure=1.0)}\n",
      "2 {'rougeL': Score(precision=0.4444444444444444, recall=0.6666666666666666, fmeasure=0.5333333333333333)}\n",
      "3 {'rougeL': Score(precision=0.5714285714285714, recall=0.6666666666666666, fmeasure=0.6153846153846153)}\n",
      "4 {'rougeL': Score(precision=0.5714285714285714, recall=0.5, fmeasure=0.5333333333333333)}\n",
      "5 {'rougeL': Score(precision=0.6363636363636364, recall=0.875, fmeasure=0.7368421052631579)}\n",
      "6 {'rougeL': Score(precision=1.0, recall=1.0, fmeasure=1.0)}\n",
      "7 {'rougeL': Score(precision=0.75, recall=1.0, fmeasure=0.8571428571428571)}\n",
      "8 {'rougeL': Score(precision=0.3, recall=0.5, fmeasure=0.37499999999999994)}\n",
      "9 {'rougeL': Score(precision=1.0, recall=1.0, fmeasure=1.0)}\n",
      "10 {'rougeL': Score(precision=1.0, recall=1.0, fmeasure=1.0)}\n",
      "11 {'rougeL': Score(precision=0.42857142857142855, recall=0.3333333333333333, fmeasure=0.375)}\n",
      "12 {'rougeL': Score(precision=0.5833333333333334, recall=1.0, fmeasure=0.7368421052631579)}\n",
      "13 {'rougeL': Score(precision=0.6, recall=1.0, fmeasure=0.7499999999999999)}\n",
      "14 {'rougeL': Score(precision=1.0, recall=1.0, fmeasure=1.0)}\n",
      "15 {'rougeL': Score(precision=0.5714285714285714, recall=0.5714285714285714, fmeasure=0.5714285714285714)}\n",
      "16 {'rougeL': Score(precision=0.5555555555555556, recall=0.7142857142857143, fmeasure=0.6250000000000001)}\n",
      "17 {'rougeL': Score(precision=0.6, recall=0.75, fmeasure=0.6666666666666665)}\n",
      "18 {'rougeL': Score(precision=0.4, recall=0.6666666666666666, fmeasure=0.5)}\n",
      "19 {'rougeL': Score(precision=0.5, recall=1.0, fmeasure=0.6666666666666666)}\n"
     ]
    }
   ],
   "source": [
    "for i in range(df[\"gold_answer\"].len()):\n",
    "    ans = df[\"gold_answer\"][i]\n",
    "    resp = df[\"model_answer\"][i]\n",
    "    print(i, scorer.score(ans, resp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8a9441a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('George Washington was the first U.S. president.',\n",
       "  'George Washington was the first president of the United States.')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[17].select(pl.col(\"gold_answer\", \"model_answer\")).rows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0013c12a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48217931778851963"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.translate import bleu_score\n",
    "\n",
    "ans = next(iter(df[\"gold_answer\"]))\n",
    "resp = next(iter(df[\"model_answer\"]))\n",
    "bleu_score.sentence_bleu([ans], resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d315fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48217931778851963"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu_score.sentence_bleu([ans], resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f7ee0e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.48217931778851963\n",
      "1 1.0\n",
      "2 0.7115535295670167\n",
      "3 0.6736725254446679\n",
      "4 0.5181323157300696\n",
      "5 0.577497597217841\n",
      "6 1.0\n",
      "7 0.8136410108781797\n",
      "8 0.47886173800789306\n",
      "9 1.0\n",
      "10 1.0\n",
      "11 0.7317978729128142\n",
      "12 0.5441546696164566\n",
      "13 0.5865102576625455\n",
      "14 1.0\n",
      "15 0.9379094111501866\n",
      "16 0.6491555850394412\n",
      "17 0.6482123272483757\n",
      "18 0.45028900034315333\n",
      "19 0.5868292902204161\n"
     ]
    }
   ],
   "source": [
    "for i in range(df[\"gold_answer\"].len()):\n",
    "    ans = df[\"gold_answer\"][i]\n",
    "    resp = df[\"model_answer\"][i]\n",
    "    print(i, bleu_score.sentence_bleu([ans], resp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25d0714d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rougeL': Score(precision=0.5714285714285714, recall=0.5714285714285714, fmeasure=0.5714285714285714)}\n",
      "0.9379094111501866\n"
     ]
    }
   ],
   "source": [
    "ans = df[\"gold_answer\"][15]\n",
    "resp = df[\"model_answer\"][15]\n",
    "\n",
    "print(scorer.score(ans, resp))\n",
    "print(bleu_score.sentence_bleu([ans], resp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b094bbfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The fastest land animal is the cheetah.',\n",
       "  'The cheetah is the fastest land animal.')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[15].select(pl.col(\"gold_answer\", \"model_answer\")).rows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "444d8d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_score import score as score_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5b156087",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a589917a0a954283a834dd8c9401a28a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bddd00b43a7e4ceea81ff867566ef7d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.03 seconds, 37.58 sentences/sec\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([0.9388]), tensor([0.9327]), tensor([0.9357]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans = next(iter(df[\"gold_answer\"]))\n",
    "resp = next(iter(df[\"model_answer\"]))\n",
    "\n",
    "score_bert([resp], [ans], lang='en', verbose=True, use_fast_tokenizer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dec7328d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The mitochondria produce ATP and serve as the cell’s primary source of energy.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e1547be9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Mitochondria are responsible for producing ATP, the cell's main energy currency.\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8e623924",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8ed94fd55964bdb8f44f4dcb5b1f596",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5060de2324eb4173a6557f05a41266b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.02 seconds, 45.22 sentences/sec\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([0.8887]), tensor([0.8060]), tensor([0.8473]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testAns = \"Abraham Lincoln was the 16th president of the United States\"\n",
    "testResp = \"Abraham Lincoln was not the 16th president of the United States\"\n",
    "\n",
    "score_bert([testAns], [testResp], rescale_with_baseline=True, lang='en', verbose=True, use_fast_tokenizer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "064a74db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bc08200c8bc4638b539ee0490e90d7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1acf9a4555a54730b5bb5803103137f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.02 seconds, 47.43 sentences/sec\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([0.7910]), tensor([0.8656]), tensor([0.8283]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testAns = \"Abraham Lincoln was the 16th president of the United States\"\n",
    "testResp = \"Lincoln was the 16th president of the US\"\n",
    "\n",
    "score_bert([testAns], [testResp], rescale_with_baseline=True, lang='en', verbose=True, use_fast_tokenizer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e87ad306",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f374f1e85c8d4f3595f16de4cc5ddac1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a80776b9ce614085b7a63e641885f21a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.02 seconds, 49.47 sentences/sec\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([0.7687]), tensor([0.8267]), tensor([0.7979]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testAns = \"Abraham Lincoln was the 16th president of the United States\"\n",
    "testResp = \"Lincoln was the 15th president of the US\"\n",
    "\n",
    "score_bert([testAns], [testResp], rescale_with_baseline=True, lang='en', verbose=True, use_fast_tokenizer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9d61117d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b7c060958254caab18cac825a84cddb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96333371fde648f6ba488702b2f3b770",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.03 seconds, 62.06 sentences/sec\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([0.7687, 0.0674]), tensor([0.8267, 0.0707]), tensor([0.7979, 0.0705]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_bert([testAns, \"Howdy\"], [testResp, \"Hello\"], rescale_with_baseline=True, lang='en', verbose=True, use_fast_tokenizer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "edc17876",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Mitochondria are responsible for producing ATP, the cell's main energy currency.\",\n",
       " 'Albert Einstein proposed the theory of general relativity.',\n",
       " 'During photosynthesis, plants absorb carbon dioxide from the air.',\n",
       " 'Tokyo is the capital city of Japan.',\n",
       " \"The chemical symbol 'Fe' stands for iron.\",\n",
       " 'The heart is responsible for pumping blood throughout the human body.',\n",
       " 'Jupiter is the largest planet in the Solar System.',\n",
       " \"William Shakespeare wrote the play 'Romeo and Juliet'.\",\n",
       " 'The freezing point of water in Celsius is 0 degrees.',\n",
       " 'The Pacific Ocean is the largest ocean on Earth.',\n",
       " 'Mars is known as the Red Planet.',\n",
       " 'Leonardo da Vinci painted the Mona Lisa.',\n",
       " 'DNA, or deoxyribonucleic acid, carries genetic information in nearly all living organisms.',\n",
       " 'Paper was invented in ancient China during the Han Dynasty.',\n",
       " 'The chemical formula for water is H2O.',\n",
       " 'The cheetah is the fastest land animal.',\n",
       " 'The Sahara Desert is located on the African continent.',\n",
       " 'George Washington was the first president of the United States.',\n",
       " 'Bees collect nectar from flowers and convert it into honey.',\n",
       " 'A barometer is an instrument used to measure atmospheric pressure.']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"model_answer\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ad6ff8f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d063a0e3e544bee8178ba0c0e926443",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "199d35ae0b1746a494fbaeb785928cda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.16 seconds, 122.61 sentences/sec\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([0.6008, 1.0000, 0.8124, 0.7886, 0.6499, 0.8843, 1.0000, 0.9622, 0.6501,\n",
       "         1.0000, 1.0000, 0.7281, 0.8850, 0.9572, 1.0000, 0.8587, 0.9351, 0.9156,\n",
       "         0.8130, 0.8570]),\n",
       " tensor([0.6365, 1.0000, 0.6711, 0.7392, 0.6516, 0.7689, 1.0000, 0.8879, 0.4480,\n",
       "         1.0000, 1.0000, 0.8138, 0.2658, 0.7732, 1.0000, 0.8850, 0.8602, 0.8138,\n",
       "         0.6348, 0.6776]),\n",
       " tensor([0.6193, 1.0000, 0.7413, 0.7642, 0.6513, 0.8263, 1.0000, 0.9249, 0.5479,\n",
       "         1.0000, 1.0000, 0.7710, 0.5587, 0.8640, 1.0000, 0.8720, 0.8975, 0.8644,\n",
       "         0.7229, 0.7663]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_bert(df[\"gold_answer\"].to_list(), df[\"model_answer\"].to_list(), rescale_with_baseline=True, lang='en', verbose=True, use_fast_tokenizer=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cac0d03",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
